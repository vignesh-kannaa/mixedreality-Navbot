# Indoor Navigation with AI and AR
In a world where outdoor navigation has been significantly enhanced by GPS and mapping technologies, the complexities of indoor navigation often go overlooked. Indoor spaces like shopping malls, hospitals, airports, and museums present unique challenges for effective guidance. The existing indoor navigation apps primarily offer static 2D maps and minimal features, leaving users to decipher complex layouts. This becomes particularly daunting for middle-aged and elderly individuals.

However, the advent of Extended Reality (XR), encompassing Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality (MR), has ushered in transformative possibilities. Within XR, Augmented Reality stands out for introducing Humanoid Virtual Agents (HVAs), bridging the gap between the virtual and physical worlds. These HVAs, powered by Artificial Intelligence (AI), have the potential to revolutionize indoor navigation.

This project doesn't just address indoor navigation issues; it explores the integration of Virtual Agents in augmenting indoor navigation through AR. The goal is to develop a user-centric, engaging, and efficient indoor navigation system using AR and AI-driven virtual characters. The motivation behind this project arises from the pressing need for improved indoor navigation. As urbanization continues and complex buildings multiply, navigating indoor spaces becomes progressively challenging. The fusion of AR, AI, and Virtual Agents offers the potential to make indoor navigation more intuitive, informative, and enjoyable.

This project represents a comprehensive and intricate approach to indoor navigation, incorporating a wide range of software, tools, libraries, and programming languages. At its core, the utilization of 3D modelling software is pivotal, providing the foundation for mapping complex indoor environments. Virtual characters are introduced to enhance the user experience by offering guidance and interactivity within these spaces. Python facilitates Natural Language Processing (NLP) techniques, enabling effective communication between users and the system. Unity plays a crucial role in pathfinding and ensures efficient route planning, while also enabling realistic virtual character interactions through lipsyncing. The integration of Azure's cognitive services further enriches user interactions with functionalities like text-to-speech and speech-to-text conversion. To ensure scalability and accessibility, Microsoft's Azure cloud platform is leveraged for deployment, guaranteeing consistent performance as the user base grows. The project also actively utilizes Application Programming Interfaces (APIs) to facilitate seamless communication between various components, such as Hololens devices and Azure web services for NLP integration. This interoperability enhances the project's adaptability and flexibility.
## System Architecture
<img style="width:750px;" src="https://github.com/vignesh-kannaa/mixedreality-Navbot/assets/67087280/55d65a85-ae19-48c7-9822-b8cdb60f7763" alt="System Architecture">
<img style="width:750px;" src="https://github.com/vignesh-kannaa/mixedreality-Navbot/assets/67087280/5e9bd230-6bad-4366-89e7-a48bf2a2e217" alt="Softwares">

## Virtual Agent - LUNA
<img style="width:550px;" src="https://github.com/vignesh-kannaa/mixedreality-Navbot/assets/67087280/69bb01e6-6a16-41b3-b3fc-6213c97fee48" alt="Virtual Agent">
<img style="width:550px;" src="https://github.com/vignesh-kannaa/mixedreality-Navbot/assets/67087280/a456406b-de7b-4e57-8ab6-1428b058c34d" alt="Virtual Agent">
